\documentclass[twoside]{article}

\usepackage{mathpazo}
\usepackage{twemojis}

% Paper mode options:
%   - (default/preprint): Shows "Work in progress" footnote
%   - [final]: Hides "Work in progress" footnote (use when paper is ready)
%
\usepackage{manuscript}
% \usepackage[final]{manuscript}

% If you set papersize explicitly, activate the following three lines:
% \special{papersize = 8.5in, 11in}
% \setlength{\pdfpageheight}{11in}
% \setlength{\pdfpagewidth}{8.5in}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{xcolor}
\usepackage{amsmath,amsfonts,amsthm,bm}
\usepackage{cleveref}
\usepackage{thm-restate}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{multicol}
\usepackage{multirow}
\usepackage[percent]{overpic}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{listings}
\usepackage[round]{natbib}


\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{assumption}{Assumption}
\newtheorem{definition}{Definition}
\newtheorem{example}[definition]{Example}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}


\captionsetup{font=footnotesize}
\renewcommand{\bibname}{References}
\renewcommand{\bibsection}{\hspace{1em}\section*{\bibname}}

\bibliographystyle{apalike}
\input{./commands.tex}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor=blue
}


\lstset{
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{green!50!black},
    stringstyle=\color{red!50!black},
    numbers=left,
    numberstyle=\tiny\color{gray},
    numbersep=5pt,
    breaklines=true,
    frame=single,
    framesep=5pt,
    xleftmargin=15pt,
    language=Python,
    showstringspaces=false
}

% for metadata
\title{SAD: Seed-based Neural Architecture Search}
\author{Not Ayoub Not Ghriss}

% Define authors with institutions and emails for display
\addauthor{Not Ayoub Not Ghriss}{Department of Computher Thienthe\\Y'all University}{not.ayoub.ghriss@y'all.edu}

\algblockdefx[Try]{Try}{EndTry}{\textbf{try}}{\textbf{end try}}
\algblockdefx[Catch]{Catch}{EndCatch}[1]{\textbf{catch} #1}{\textbf{end catch}}

\begin{document}

\maketitle

\begin{abstract}
  Despite the proliferation of automated machine learning (AutoML) and Neural Architecture Search
  (NAS), the field remains plagued by the ``reproducibility crisis'' and the sheer cost of Z99999
  compute cycles. In this work, we introduce \textbf{S}eed-based Neural \textbf{A}rchitecture
  \textbf{S}earch (\textbf{SAD}), a framework that formalizes the industry-standard practice of
  changing the random seed until the validation accuracy reaches an arbitrary threshold for
  publication. Unlike traditional NAS, which optimizes weights $\theta$ and architectures $\alpha$,
  \textbf{SAD} acknowledges that the only variable that truly matters is the integer passed to
  \texttt{torch.manual\_seed()}. Our approach achieves State-of-the-Art (SOTA) results on
  CIFAR-10$^5$ using a single-layer perceptron, provided the seed is precisely
  $42,096$~\twemoji{winking face}. We believe this work, originally pioneered by \cite{myself},
  represents the final frontier of gradient-free optimization.
\end{abstract}

\section{Introduction}

Modern Deep Learning has become a race for computational dominance. However, as noted in the
seminal work of \cite{myself}, the most significant improvements in empirical performance often
stem not from architectural innovations, but from ``lucky'' weight initializations. Current methods
like Light Speed Architecture Search (LARTS) or Quantum Policy Optimization (QPO) require massive
QPU clusters and several PhD lifetimes to converge.

In contrast, \textbf{SAD} offers a more sustainable path: rather than searching for a robust
architecture that works across all initializations, we search for the one initialization that makes
a mediocre architecture work perfectly. This paradigm shift reduces the problem of \emph{Deep
  Learning} to \emph{Large Integer Search}, a field much better understood by our ancestors in the
21st century.

\section{Preliminaries}
Let $\mathcal{A}$ be the space of all possible neural architectures and $\mathcal{S} \subset
  \mathbb{Z}$ be the space of 128-bit unsigned integers. Traditionally, the goal of NAS is to find an
optimal architecture $\alpha^* \in \mathcal{A}$ such that:
\begin{equation}
  \alpha^* = \arg \min_{\alpha \in \mathcal{A}} \mathbb{E}_{s \sim \mathcal{S}} [ \mathcal{L}(f(x; \theta, \alpha, s)) ]
\end{equation}
where $s$ is the random seed. \textbf{SAD} rejects this expectation-based approach as ``too expensive and mathematically pessimistic.'' Instead, we define the \textbf{SAD Objective}:
\begin{equation}
  s^* = \arg \max_{s \in \mathcal{S}} \mathbb{P}(\text{Loss} < \epsilon \mid \alpha_{\text{baseline}}, s)
\end{equation}
where $\epsilon$ is the maximum loss tolerated by a reviewer who is skim-reading the paper.

\section{The SAD Method}

The \textbf{SAD} pipeline is optimized for the Year 2200 compute environment, where energy is
scarce but random integers are plentiful.

\subsection{Phase I: The Superstitious Initialization}
We begin by selecting a set of ``lucky'' seeds. Common choices include the author's birthday, the
date of the submission deadline, or the number of remaining credits on the lab's AWS account.

\subsection{Phase II: Stochastic Brute Force}
The algorithm enters a loop, as shown in Algorithm \ref{alg:sad}. We iteratively evaluate a fixed,
sub-optimal architecture (typically a ResNet-A9 with all the skip connections accidentally
commented out) across the selected seed space.

\begin{algorithm*}[ht]
  \caption{SAD: random Seed-based neural Architecture Search}
  \label{alg:sad}
  \begin{algorithmic}[1]
    \Procedure{SADSearch}{$\alpha, \mathcal{D}, \text{Budget}$}
    \State $s^* \gets 42$ \Comment{The Universal Constant}
    \State $\text{Acc}_{\text{best}} \gets -\infty$
    \State $\text{Attempts} \gets 0$

    \While{not DeadlineApproaching() \textbf{and} LabCreditsRemaining()}
    \State $s \gets \text{GenerateStochasticInteger}()$
    \State $\mathcal{M} \gets \text{InitializeWeights}(\alpha, \text{seed}=s)$

    \Try
    \State $\text{Acc}_{\text{val}} \gets \text{Train}(\mathcal{M}, \mathcal{D})$
    \If{$\text{Acc}_{\text{val}} > \text{Acc}_{\text{best}}$}
    \State $\text{Acc}_{\text{best}} \gets \text{Acc}_{\text{val}}$
    \State $s^* \gets s$
    \State \textbf{save} $\mathcal{M}$ \Comment{Checkpoint before gradient explodes}
    \EndIf
    \Catch{\textit{Out Of Memory Error}}
    \State $\text{ReduceBatchSize}()$ \Comment{Standard PhD opt step}
    \EndCatch
    \EndTry

    \State $\text{Attempts} \gets \text{Attempts} + 1$
    \EndWhile

    \State \textbf{return} $s^*, \text{Acc}_{\text{best}}$ \Comment{Discard failed seeds}
    \EndProcedure
  \end{algorithmic}
\end{algorithm*}

\paragraph{Convergence Criteria:} The search terminates immediately once the validation curve crosses the baseline of a paper from
2197.
\paragraph{Early Stopping:} If the QPU temperature exceeds 1900Â°C or the author starts crying, the search is halted and the
best result found so far is declared a ``theoretical breakthrough.''

\subsection{Phase III: Post-Hoc Justification (Seed-Space Warp)}
Rather than traversing the loss landscape via backpropagation; a method considered ``quaint'' by
modern standards; we employ \textbf{Seed-Space Warp (SSW)}. We argue that the optimal model weights
already exist in the Hilbert space of initialization; we simply need to find the integer $s^*$ that
indexes them.

Once a seed produces a lucky result, our algorithm generates a 50-page synthetic proof in
Coq-Quantum to convince reviewers that the architecture's success is due to a ``novel manifestation
of non-Euclidean manifold structure'' rather than sheer luck.

\section{Experiments}
We evaluated \textbf{SAD} against the \textbf{Hyper-Transformer-v12} on the CIFAR-10$^5$ dataset
(including 10 billion images of extinct felines).

\subsection{Visualization Failure Analysis}
As seen in Figure \ref{fig:broken_glory}, we attempted to visualize the loss landscape.

\begin{figure}[h]
  \centering
  \fbox{
    \begin{minipage}{0.45\textwidth}
      \centering
      \vspace{2cm}
      \textbf{\textcolor{red}{CRITICAL ERROR: QSL Certificate Expired}} \\
      \vspace{0.5cm}
      \textit{Cannot load holographic asset: \texttt{fig\_sota\_results.qimg}} \\
      \small{Error Code: 0xDEADC0DE - Quantum Handshake Failed}
      \vspace{2cm}
    \end{minipage}
  }
  \caption{Comparison of SAD vs. LARTS. Although the visualization subsystem failed to render the 4D-hyperplot due to a QSL Certificate error, the underlying tensor data clearly indicates that SAD achieves 99.9\% accuracy.}\label{fig:broken_glory}
\end{figure}

\subsection{Results on the Dyson Cluster}
As shown in Table \ref{tab:results}, \textbf{SAD} outperforms all baselines. While LARTS required
the energy output of a small star to reach 92\% accuracy, \textbf{SAD} reached 99\% accuracy using
the energy equivalent of a single synthetic espresso.

\begin{table*}[t]
  \centering
  \caption{Performance comparison on the Mars-Net Benchmark.}
  \label{tab:results}
  \begin{tabular}{lrrr}
    \toprule
    \textbf{Method}           & \textbf{Compute (Zetaflops)} & \textbf{CO$_2$ (Metric Tons)} & \textbf{Top-1 Acc (\%)} \\
    \midrule
    Vanilla Hyper-Transformer & $10^{15}$                    & $50,000$                      & $84.2$                  \\
    LARTS                     & $10^{18}$                    & $2,400,000$                   & $91.5$                  \\
    \textbf{SAD (Ours)}       & \textbf{42.0}                & \textbf{0.001}                & \textbf{99.9*}          \\
    \bottomrule
  \end{tabular}
  \begin{flushleft}
    \footnotesize \textit{*Performance achieved on seed 42,0\~9\~6. Results may vary if the user is having a bad day.}
  \end{flushleft}
\end{table*}

\section{Related Work}

Early attempts at architecture search, such as \textbf{LARTS} (Light Speed Architecture Search)
\citep{larts}, relied on the naive assumption that more compute equals better science. These
methods often utilized Grid Search, a primitive technique where researchers burned the fossilized
remains of prehistoric lizards to check every possible combination of hyperparameters. While
effective, LARTS was deemed illegal in 2195 under the Clean Energy Act. Our method, \textbf{SAD},
achieves similar results with zero carbon footprint, assuming one ignores the mental energy
expended by the author guessing seeds.

\subsection{Quantum Hallucinations}
More recently, Quantum Policy Optimization (QPO) attempted to solve the NAS problem by placing the
neural network weights in a superposition of being both \emph{converged} and \emph{diverged}
simultaneously \citep{schrodingers_net}. Proponents argued that by training on a QPU (Quantum
Processing Unit), one could explore all possible loss landscapes at once.

However, QPO suffers from a critical theoretical flaw known as the \textit{Heisenberg Uncertainty
  Principle of Publication}: as soon as a reviewer attempts to measure the validation accuracy, the
wave function collapses, and the model performance drops to random guessing (10\%). Furthermore,
QPO requires cooling the hardware to near absolute zero ($0^{\circ}$K), whereas \textbf{SAD} runs
efficiently at standard room temperature, or slightly higher if cooling vents are clogged with dead
nanobots.

\subsection{Randomness as a Feature}
Our work is most closely related to the manifesto of \cite{myself}, which first proposed that the
"Global Minimum" is a myth invented to sell gradient descent optimizers. We extend this theory by
demonstrating that specific integers (e.g., 42,096, 1337) possess intrinsic inductive biases that
outperform decades of hand-crafted architectural engineering.
\section{Conclusion}
In this paper, we have proven that the ``Learning'' in Deep Learning is largely a historical
misunderstanding. By utilizing \textbf{SAD}, researchers can return to what truly matters: staring
at the loss curve until it does something interesting. As we move toward the 26rd century, we
expect \textbf{SAD} to become the primary method for all researchers who value their sleep over
their QPU quotas.

\bibliography{references}

\clearpage
\appendix
\thispagestyle{empty}
\onecolumn
\displaytitle{Appendices}

\input{./appendix.tex}

\end{document}
